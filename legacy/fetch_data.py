# fetch_data.py
# ä» OKX API è·å–å¤šä¸ªå¸ç§çš„ K çº¿æ•°æ®å¹¶å•ç‹¬ä¿å­˜
# ä¾èµ–: requests, pandas, argparse (è¯·å…ˆå®‰è£…: pip install requests pandas)
#usage: python fetch_data.py --update ä¸»è¦ç”¨è¿™ä¸ªå°±è¡Œï¼Œé—´æ–­æ—¶é—´ä¸è¶…è¿‡5å¤©å³å¯ï¼Œå¯ä»¥è€ƒè™‘æŒ‚åœ¨åå°
import requests
import pandas as pd
import time
import os
import json
import sys
import argparse

# --- é…ç½® ---
# è¦è·å–çš„å¸ç§åˆ—è¡¨
SYMBOLS = ['CAT','BTC','DEGEN','DOGE','BONK','ELON','NOT','PEPE','PNUT','SHIB','SLERF','TRUMP','WIF','XRP','ETH','SOL','TRX','SUI','XLM','LINK','MEW','ONDO','ADA','AAVE','AVAX']

# OKX API ç›¸å…³
API_ENDPOINT = "https://www.okx.com/api/v5/market/candles"
BAR = "5m" # Kçº¿å‘¨æœŸ
LIMIT = 300 # æ¯æ¬¡ API è¯·æ±‚è·å–çš„æœ€å¤§ K çº¿æ¡æ•°
MAX_PAGES = 100 # æ¯ä¸ªå¸ç§æœ€å¤šè·å–çš„é¡µæ•°
REQUEST_DELAY = 0.2 # è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰
REQUEST_TIMEOUT = 15 # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# æ•°æ®ä¿å­˜ç›®å½•
OUTPUT_DIR = "okx_data"

# --- å‡½æ•°å®šä¹‰ ---
def fetch_okx_5m_data(symbol, bar=BAR, max_pages=MAX_PAGES, limit=LIMIT):
    """æ‹‰å–å•ä¸ªå¸ç§çš„OKX Kçº¿æ•°æ®ï¼Œæ”¯æŒç¿»é¡µã€‚"""
    inst_id = f"{symbol}-USDT"
    all_data = []
    after = None  # ç”¨äºç¿»é¡µçš„æ—¶é—´æˆ³
    print(f"  Fetching {inst_id} ({bar}, max {max_pages} pages)...", end='')
    page_count = 0
    
    for _ in range(max_pages):
        params = {
            "instId": inst_id,
            "bar": bar,
            "limit": limit
        }
        if after:
            params["after"] = after
        
        try:
            response = requests.get(API_ENDPOINT, params=params, timeout=REQUEST_TIMEOUT)
            response.raise_for_status() # æ£€æŸ¥ HTTP é”™è¯¯
            data = response.json()

            # æ£€æŸ¥ä¸šåŠ¡é”™è¯¯ç 
            if data.get("code") == "0":
                candles = data.get("data", [])
                if not candles:
                    break # æ²¡æœ‰æ›´å¤šæ•°æ®äº†
                # æ£€æŸ¥ç¬¬ä¸€æ¡æ•°æ®çš„åˆ—æ•°æ˜¯å¦ä¸º9 (åªæ£€æŸ¥ä¸€æ¬¡)
                if page_count == 0 and len(candles[0]) != 9:
                     print(f"\n  âŒ Error: API for {symbol} did not return 9 columns as expected. Got {len(candles[0])}. Stopping fetch.")
                     return pd.DataFrame() # è¿”å›ç©º DF
                all_data.extend(candles)
                # OKX è¿”å›çš„æ—¶é—´æˆ³æ˜¯å‡åºçš„ï¼Œafteréœ€è¦ç”¨æœ€æ–°çš„æ—¶é—´æˆ³(åˆ—è¡¨æœ€åä¸€ä¸ª)
                after = candles[-1][0] 
                page_count += 1
                time.sleep(REQUEST_DELAY) # å°Šé‡é€Ÿç‡é™åˆ¶
            else:
                print(f"\n  âŒ OKX API Error for {symbol}: {data.get('msg')} (Code: {data.get('code')})")
                break # APIè¿”å›é”™è¯¯ï¼Œåœæ­¢è¯¥å¸ç§çš„è·å–

        except requests.exceptions.Timeout:
            print(f"\n  âŒ Timeout during request for {symbol}. Stopping fetch.")
            break 
        except requests.exceptions.RequestException as e:
            print(f"\n  âŒ Request failed for {symbol}: {e}. Stopping fetch.")
            break
        except json.JSONDecodeError:
            print(f"\n  âŒ JSON Decode Error for {symbol}. Stopping fetch.")
            break
        except Exception as e:
             print(f"\n  âŒ An unexpected error occurred for {symbol}: {e}. Stopping fetch.")
             break
             
    print(f" Done ({page_count} pages, {len(all_data)} candles).")

    if not all_data:
        return pd.DataFrame() # å¦‚æœè·å–å¤±è´¥æˆ–æ— æ•°æ®ï¼Œè¿”å›ç©ºDataFrame

    # å®šä¹‰å®Œæ•´çš„9åˆ—åç§° (æ ¹æ®OKX V5æ–‡æ¡£)
    columns = [
        "timestamp", "open", "high", "low", "close", "volume_base",
        "volume_quote", "volume_quote_2", "confirm" # å‡è®¾ç¬¬8åˆ—æ˜¯ volume_quote çš„å¦ä¸€ç§è¡¨ç¤º
    ]
    # ç¡®ä¿æ•°æ®æœ‰9åˆ—æ‰åˆ›å»ºDataFrame
    if len(all_data[0]) != 9:
         print(f"\n  âŒ Error: Data for {symbol} does not have 9 columns after fetch. Cannot create DataFrame.")
         return pd.DataFrame()
         
    df = pd.DataFrame(all_data, columns=columns)
    
    df["timestamp"] = pd.to_datetime(df["timestamp"].astype(float), unit="ms")
    
    # è½¬æ¢æ•°å€¼åˆ— (åŒ…æ‹¬æ–°å¢çš„ volume_quote_2)
    numeric_cols = ['open', 'high', 'low', 'close', 'volume_base', 'volume_quote', 'volume_quote_2', 'confirm']
    for col in numeric_cols:
        if col in df.columns: # Check if column exists before converting
             df[col] = pd.to_numeric(df[col], errors='coerce')
        
    # é€šå¸¸APIè¿”å›æ˜¯æ—¶é—´é™åºï¼Œè¿™é‡Œè·å–åæŒ‰æ—¶é—´å‡åºæ’
    df = df.sort_values("timestamp").reset_index(drop=True)
    # å¯é€‰ï¼šå»é‡ï¼Œè™½ç„¶åˆ†é¡µç†è®ºä¸Šä¸åº”é‡å¤ï¼Œä½†ä¿é™©èµ·è§
    df = df.drop_duplicates(subset=['timestamp'], keep='first') 
    return df

# --- å‘½ä»¤è¡Œå‚æ•° ---
def parse_arguments():
    parser = argparse.ArgumentParser(description='ä»OKXè·å–Kçº¿æ•°æ®')
    parser.add_argument('--update', action='store_true', 
                        help='æ›´æ–°æ¨¡å¼ï¼šæ·»åŠ æ–°æ•°æ®åˆ°ç°æœ‰æ–‡ä»¶å¹¶ç§»é™¤æœªç¡®è®¤(confirm=0)çš„Kçº¿')
    return parser.parse_args()

# --- ä¸»ç¨‹åº ---
if __name__ == "__main__":
    args = parse_arguments()
    update_mode = args.update
    
    if update_mode:
        print(f"Starting data update for {len(SYMBOLS)} symbols (update mode)...")
    else:
        print(f"Starting data fetch for {len(SYMBOLS)} symbols (new data mode)...")
    
    os.makedirs(OUTPUT_DIR, exist_ok=True) # åˆ›å»ºè¾“å‡ºç›®å½•

    total_symbols_processed = 0
    total_symbols_saved = 0

    for symbol in SYMBOLS:
        print(f"\nğŸ“¥ Processing symbol: {symbol}")
        df_new = fetch_okx_5m_data(symbol, bar=BAR, max_pages=MAX_PAGES, limit=LIMIT)
        total_symbols_processed += 1

        if not df_new.empty:
            filename = os.path.join(OUTPUT_DIR, f"{symbol}-USDT_{BAR}_raw.csv")
            
            if update_mode and os.path.exists(filename):
                try:
                    # è¯»å–ç°æœ‰æ•°æ®
                    df_existing = pd.read_csv(filename)
                    
                    # è½¬æ¢æ—¶é—´æˆ³åˆ—ä¸ºdatetimeä»¥ä¾¿åˆå¹¶
                    if 'timestamp' in df_existing.columns:
                        df_existing['timestamp'] = pd.to_datetime(df_existing['timestamp'])
                    
                    # åˆå¹¶æ–°æ—§æ•°æ®
                    df_combined = pd.concat([df_existing, df_new], ignore_index=True)
                    
                    # å»é‡
                    df_combined = df_combined.drop_duplicates(subset=['timestamp'], keep='last')
                    
                    # ç§»é™¤æ‰€æœ‰confirm=0çš„è¡Œ
                    df_filtered = df_combined[df_combined['confirm'] != 0]
                    
                    # æŒ‰æ—¶é—´æ’åº
                    df_filtered = df_filtered.sort_values("timestamp").reset_index(drop=True)
                    
                    # ä¿å­˜æ›´æ–°åçš„æ•°æ®
                    df_filtered.to_csv(filename, index=False, encoding='utf-8')
                    print(f"  ğŸ’¾ Updated data saved to {filename}")
                    print(f"      åŸå§‹è¡Œæ•°: {len(df_existing)}, æ–°å¢è¡Œæ•°: {len(df_new)}")
                    print(f"      åˆå¹¶å: {len(df_combined)}, è¿‡æ»¤å: {len(df_filtered)}")
                    total_symbols_saved += 1
                    
                except Exception as e:
                    print(f"  âŒ Failed to update data for {symbol}: {e}", file=sys.stderr)
            else:
                # æ–°æ¨¡å¼æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥ä¿å­˜æ–°æ•°æ®
                try:
                    df_new.to_csv(filename, index=False, encoding='utf-8')
                    print(f"  ğŸ’¾ Raw data saved to {filename} ({len(df_new)} rows)")
                    total_symbols_saved += 1
                except Exception as e:
                    print(f"  âŒ Failed to save raw data for {symbol}: {e}", file=sys.stderr)
        else:
             print(f"  âš ï¸ No data fetched or processed for {symbol}.")

    print(f"\n--- {'Update' if update_mode else 'Fetch'} complete ---")
    print(f"Processed: {total_symbols_processed} symbols")
    print(f"Successfully saved: {total_symbols_saved} files to '{OUTPUT_DIR}'")
